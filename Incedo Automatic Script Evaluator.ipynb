{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data\n",
    "train = pd.read_csv('Data/train_dataset.csv')\n",
    "test = pd.read_csv('Data/test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             int64\n",
       "Essayset     float64\n",
       "min_score      int64\n",
       "max_score      int64\n",
       "score_1        int64\n",
       "score_2        int64\n",
       "score_3      float64\n",
       "score_4      float64\n",
       "score_5      float64\n",
       "clarity       object\n",
       "coherent      object\n",
       "EssayText     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset has total 12 columns\n",
    "#out of which 3 has text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Essayset</th>\n",
       "      <th>min_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>score_4</th>\n",
       "      <th>score_5</th>\n",
       "      <th>clarity</th>\n",
       "      <th>coherent</th>\n",
       "      <th>EssayText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>average</td>\n",
       "      <td>worst</td>\n",
       "      <td>Some additional information that we would need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>excellent</td>\n",
       "      <td>worst</td>\n",
       "      <td>After reading the expirement, I realized that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>worst</td>\n",
       "      <td>above_average</td>\n",
       "      <td>What you need is more trials, a control set up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>worst</td>\n",
       "      <td>worst</td>\n",
       "      <td>The student should list what rock is better an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>above_average</td>\n",
       "      <td>worst</td>\n",
       "      <td>For the students to be able to make a replicat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Essayset  min_score  max_score  score_1  score_2  score_3  score_4  \\\n",
       "0   1       1.0          0          3        1        1      1.0      1.0   \n",
       "1   2       1.0          0          3        1        1      NaN      1.5   \n",
       "2   3       1.0          0          3        1        1      1.0      1.0   \n",
       "3   4       1.0          0          3        0        0      0.0      0.0   \n",
       "4   5       1.0          0          3        2        2      2.0      2.5   \n",
       "\n",
       "   score_5        clarity       coherent  \\\n",
       "0      1.0        average          worst   \n",
       "1      1.0      excellent          worst   \n",
       "2      1.5          worst  above_average   \n",
       "3      1.0          worst          worst   \n",
       "4      1.0  above_average          worst   \n",
       "\n",
       "                                           EssayText  \n",
       "0  Some additional information that we would need...  \n",
       "1  After reading the expirement, I realized that ...  \n",
       "2  What you need is more trials, a control set up...  \n",
       "3  The student should list what rock is better an...  \n",
       "4  For the students to be able to make a replicat...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look like there's some missing value..let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Essayset</th>\n",
       "      <td>157</td>\n",
       "      <td>0.921199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_3</th>\n",
       "      <td>147</td>\n",
       "      <td>0.862524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coherent</th>\n",
       "      <td>145</td>\n",
       "      <td>0.850789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_5</th>\n",
       "      <td>144</td>\n",
       "      <td>0.844922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clarity</th>\n",
       "      <td>138</td>\n",
       "      <td>0.809717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_4</th>\n",
       "      <td>136</td>\n",
       "      <td>0.797982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EssayText</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_score</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_score</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Total   Percent\n",
       "Essayset     157  0.921199\n",
       "score_3      147  0.862524\n",
       "coherent     145  0.850789\n",
       "score_5      144  0.844922\n",
       "clarity      138  0.809717\n",
       "score_4      136  0.797982\n",
       "EssayText      0  0.000000\n",
       "score_2        0  0.000000\n",
       "score_1        0  0.000000\n",
       "max_score      0  0.000000\n",
       "min_score      0  0.000000\n",
       "ID             0  0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missing data\n",
    "total = train.isnull().sum().sort_values(ascending=False)\n",
    "percent = ((train.isnull().sum()/ train.isnull().count()) * 100).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6-columns have missing data\n",
    "\n",
    "#filling essasy set\n",
    "train['Essayset'] = train['Essayset'].fillna(train['Essayset'].mode()[0])\n",
    "\n",
    "#filling missing scores\n",
    "train['score_1'] = train['score_1'].fillna(train['score_1'].mode()[0])\n",
    "train['score_2'] = train['score_2'].fillna(train['score_2'].mode()[0])\n",
    "train['score_3'] = train['score_3'].fillna(train['score_3'].mode()[0])\n",
    "train['score_4'] = train['score_4'].fillna(train['score_4'].mode()[0])\n",
    "train['score_5'] = train['score_5'].fillna(train['score_5'].mode()[0])\n",
    "\n",
    "#filling clarity and coherent\n",
    "train['clarity'] = train['clarity'].fillna(train['score_5'].mode()[0])\n",
    "train['coherent'] = train['coherent'].fillna(train['score_5'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0\n",
       "Essayset     0\n",
       "min_score    0\n",
       "max_score    0\n",
       "score_1      0\n",
       "score_2      0\n",
       "score_3      0\n",
       "score_4      0\n",
       "score_5      0\n",
       "clarity      0\n",
       "coherent     0\n",
       "EssayText    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check if still there's any missing value\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our targer variable is the avg score given to essay by 5 evulator\n",
    "#but in trainging data we have 5 diff scores\n",
    "#so we need to make a new var of avg score which we will use as target var\n",
    "train['AvgScore'] = (train['score_1'] + train['score_2'] + train['score_3'] + train['score_4'] + train['score_5']) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our target var\n",
    "y = train['AvgScore'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Some additional information that we would need...\n",
       "1    After reading the expirement, I realized that ...\n",
       "2    What you need is more trials, a control set up...\n",
       "3    The student should list what rock is better an...\n",
       "4    For the students to be able to make a replicat...\n",
       "Name: EssayText, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check for the essasy text\n",
    "train['EssayText'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see that its contain stopwords and punct so we need to first clean this text\n",
    "\n",
    "#function to clean raw text using nlp\n",
    "def clean_text(raw_text):\n",
    "    #remove spaces\n",
    "    raw_text=raw_text.strip()\n",
    "    #remove punct\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text) \n",
    "    #lower caseing words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #stopwords removal\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    #stemming\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    meaningful_words1=[stemmer.stem(word) for word in meaningful_words]\n",
    "    \n",
    "\n",
    "    return(\" \".join(meaningful_words1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len of essay text before cleaning\n",
    "train['EssayText_before_len']=train['EssayText'].apply(len)\n",
    "#cleaning essay text\n",
    "train['EssayText']=train['EssayText'].apply(clean_text)\n",
    "#len of essay text before cleaning\n",
    "train['EssayText_after_len']=train['EssayText'].apply(len)\n",
    "\n",
    "#no of stopwords,punc removed\n",
    "train['Stopword_Used'] = train['EssayText_before_len'] - train['EssayText_after_len']\n",
    "\n",
    "#we can use this feature in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert categorical text var to numerical dummy vars\n",
    "train=pd.get_dummies(train,columns=['clarity','coherent'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see no of features we have for traing\n",
    "feats =['max_score', 'EssayText_before_len', 'EssayText_after_len',\n",
    "        'clarity_average', 'clarity_excellent','clarity_worst', \n",
    "        'coherent_average', 'coherent_excellent','coherent_worst']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using tf idf or count Vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "vec_cs = TfidfVectorizer(ngram_range=(1,3),stop_words=\"english\", analyzer='word')\n",
    "consum_comp_sum = vec_cs.fit_transform(train['EssayText'])\n",
    "\n",
    "vec_cs_char = TfidfVectorizer(ngram_range=(1,9),stop_words=\"english\", analyzer='char')\n",
    "consum_comp_sum_char =vec_cs_char.fit_transform(train['EssayText'])\n",
    "\n",
    "\n",
    "final_features = scipy.sparse.hstack((train[feats], consum_comp_sum, consum_comp_sum_char)).tocsr()\n",
    "\n",
    "\n",
    "#so now have training data ready to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0\n",
       "Essayset     0\n",
       "min_score    0\n",
       "max_score    0\n",
       "clarity      0\n",
       "coherent     0\n",
       "EssayText    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets now prepare the test data in the same way\n",
    "test.isnull().sum()\n",
    "#traing data does not have any missing value so no need to worry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same for test file\n",
    "test['EssayText_before_len']=test['EssayText'].apply(len)\n",
    "test['EssayText']=test['EssayText'].apply(clean_text)\n",
    "test['EssayText_after_len']=test['EssayText'].apply(len)\n",
    "test['Stopword_Used'] = test['EssayText_before_len'] - test['EssayText_after_len']\n",
    "\n",
    "#change Text Category to label encoding\n",
    "test=pd.get_dummies(test,columns=['clarity','coherent'],drop_first=True)\n",
    "\n",
    "consum_comp_sum_test = vec_cs.transform(test['EssayText'])\n",
    "consum_comp_sum_test_char = vec_cs_char.transform(test['EssayText'])\n",
    "\n",
    "final_features_test = scipy.sparse.hstack((test[feats], consum_comp_sum_test, consum_comp_sum_test_char)).tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hank/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/hank/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/hank/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#Apply model to predict\n",
    "#logistic \n",
    "lr=LogisticRegression(verbose=1,class_weight='balanced',C=5,random_state=1996,n_jobs=-1)\n",
    "lr.fit(final_features,y)\n",
    "lr_pred=lr.predict(final_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make Submission - id,essay_set,essay_score\n",
    "submission=pd.DataFrame(\n",
    "                {'id':test['ID'],\n",
    "                 'essay_set' :test['Essayset'],\n",
    "                 'essay_score':lr_pred}\n",
    "              )\n",
    "\n",
    "submission.head()\n",
    "submission.to_csv('submission_LR_' + datetime.datetime.now().strftime(\"%d%m%Y%H%M%S\") + '.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
